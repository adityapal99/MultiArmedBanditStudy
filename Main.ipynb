{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "interpreter": {
      "hash": "96f3de3e35cd8a510ec6db809416a3e7c6065d008df0dc06c6b6acd4d76cb071"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit ('tf_gpu': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "Main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityapal99/MultiArmedBanditStudy/blob/main/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia8dIGXAYPQf"
      },
      "source": [
        "# Multi Armed Bandit Example"
      ],
      "id": "ia8dIGXAYPQf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SRDoDqTY_Ht",
        "outputId": "8ebf7a52-d886-4e8a-99ee-f8ca72ca5c64"
      },
      "source": [
        "!pip3 install tf-agents\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.6/site-packages/')"
      ],
      "id": "5SRDoDqTY_Ht",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-agents\n",
            "  Downloading tf_agents-0.8.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.12.0)\n",
            "Requirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.17.3)\n",
            "Collecting tensorflow-probability==0.12.2\n",
            "  Downloading tensorflow_probability-0.12.2-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.8 MB 48.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.17.3)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.7.4.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.19.5)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.12.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.12.2->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.12.2->tf-agents) (0.4.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.12.2->tf-agents) (0.1.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17.0->tf-agents) (0.16.0)\n",
            "Installing collected packages: tensorflow-probability, tf-agents\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.13.0\n",
            "    Uninstalling tensorflow-probability-0.13.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.13.0\n",
            "Successfully installed tensorflow-probability-0.12.2 tf-agents-0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdsOASRaYPQi"
      },
      "source": [
        "import abc\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "from tf_agents.agents import tf_agent\n",
        "from tf_agents.drivers import driver\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.policies import tf_policy\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.trajectories import policy_step\n",
        "\n",
        "import pprint\n",
        "\n",
        "# Clear any leftover state from previous colabs run.\n",
        "# (This is not necessary for normal programs.)\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "tf.compat.v1.enable_resource_variables()\n",
        "tf.compat.v1.enable_v2_behavior()\n",
        "nest = tf.compat.v2.nest"
      ],
      "id": "ZdsOASRaYPQi",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wSQXi7vqERt"
      },
      "source": [
        "## Setting Up Python Environment using Tensorflow\n",
        "\n",
        "- Creating an Abstract Method to modify the environment based on requirements\n",
        "- Using `tf_agents.environment.py_environment.PyEnvironment` and inheriting it to create the environment."
      ],
      "id": "0wSQXi7vqERt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs3RNBaRYPQj"
      },
      "source": [
        "class AbstractBanditPyEnvironment(py_environment.PyEnvironment):\n",
        "    def __init__(self, observation_spec, action_spec):\n",
        "        self._observation_spec = observation_spec\n",
        "        self._action_spec = action_spec\n",
        "        super(AbstractBanditPyEnvironment, self).__init__()\n",
        "\n",
        "    # Helper functions.\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "\n",
        "    def _empty_observation(self):\n",
        "        return tf.nest.map_structure(lambda x: np.zeros(x.shape, x.dtype), self.observation_spec())\n",
        "\n",
        "    # These two functions below should not be overridden by subclasses.\n",
        "    def _reset(self):\n",
        "        \"\"\"Returns a time step containing an observation.\"\"\"\n",
        "        return ts.restart(self._observe(), batch_size=self.batch_size)\n",
        "\n",
        "    def _step(self, action):\n",
        "        \"\"\"Returns a time step containing the reward for the action taken.\"\"\"\n",
        "        reward = self._apply_action(action)\n",
        "        return ts.termination(self._observe(), reward)\n",
        "\n",
        "    # These two functions below are to be implemented in subclasses.\n",
        "    @abc.abstractmethod\n",
        "    def _observe(self):\n",
        "        \"\"\"Returns an observation.\"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def _apply_action(self, action):\n",
        "        \"\"\"Applies `action` to the Environment and returns the corresponding reward.\n",
        "        \"\"\"\n",
        "\n"
      ],
      "id": "Qs3RNBaRYPQj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iXRb8m_q6ya"
      },
      "source": [
        "### Custom MultiArmedBandit Implementation\n",
        "\n",
        "- Observe the method `MultiArmedBanditPyEnvironment._observe()`\n",
        "    - Here we will set the observation algorithm based on sales or something.\n",
        "    - Right now I am choosing a random observation based on price. But later more fields will be responsible for changing the `_observation` variable.\n",
        "\n",
        "- Observer the method `MultiArmedBanditPyEnvironment._apply_action()`\n",
        "    - Here we will use a formula to calculate the reward based on action and observation.\n",
        "    - Right now a very simple `max` function is being used.\n",
        "    - Later on we will use something like <strong>Softmax</strong> or <strong>Epsilon Greedy</strong>\n",
        "\n"
      ],
      "id": "9iXRb8m_q6ya"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jz2r9u1q5jw"
      },
      "source": [
        "class MultiArmedBanditPyEnvironment(AbstractBanditPyEnvironment):\n",
        "    def __init__(self, max_price: np.float64, min_price: np.float64):\n",
        "        self.max_price, self.min_price = max_price, min_price\n",
        "        action_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "        observation_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(1,), dtype=np.int32, minimum=-2, maximum=2, name='observation')\n",
        "        super(MultiArmedBanditPyEnvironment, self).__init__(observation_spec, action_spec)\n",
        "\n",
        "    def _observe(self):\n",
        "        self._observation = np.random.random() * (self.max_price - self.min_price) + self.min_price\n",
        "        return self._observation\n",
        "\n",
        "    def _apply_action(self, action):\n",
        "        return max(action, self._observation)"
      ],
      "id": "1jz2r9u1q5jw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG9pU6hIj-KF",
        "outputId": "9dbd3c60-7d7f-4773-d0b9-ccf2bc65f807"
      },
      "source": [
        "value = np.random.random()"
      ],
      "id": "QG9pU6hIj-KF",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.185117389997751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq6DCw4nfW02"
      },
      "source": [
        "df = pd.read_csv('/content/sample_data/ElectronicsProductsPricingData.csv', encoding=\"UTF-8\")\n",
        "df.head()\n",
        "\n",
        "test_cases_for_each_product = 10\n",
        "price_predictions = list()\n",
        "\n",
        "\n",
        "for row, value in df[['prices.amountMax', 'prices.amountMin']].iterrows():\n",
        "    bandit = MultiArmedBanditPyEnvironment(max_price=value.array[0], min_price=value.array[1])\n",
        "    first_obs = bandit.reset().observation\n",
        "\n",
        "    price_predictions.append({'observations': [first_obs, ], 'rewards': []})\n",
        "    for _ in range(10):\n",
        "        action = np.random.random() * (bandit.max_price - bandit.min_price) + bandit.min_price\n",
        "        result = bandit.step(action)\n",
        "        price_predictions[row]['rewards'].append(result.reward)\n",
        "        price_predictions[row]['observations'].append(result.observation)\n"
      ],
      "id": "cq6DCw4nfW02",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQWyhSMwnTQ_",
        "outputId": "449a6f53-663b-440f-8c8a-25eee68c5ebf"
      },
      "source": [
        "pprint.pprint(price_predictions[:5])"
      ],
      "id": "rQWyhSMwnTQ_",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'observations': [104.99,\n",
            "                   104.99,\n",
            "                   104.99,\n",
            "                   104.99,\n",
            "                   104.99,\n",
            "                   104.99,\n",
            "                   104.99,\n",
            "                   104.99,\n",
            "                   104.99,\n",
            "                   104.99,\n",
            "                   104.99],\n",
            "  'rewards': [array(104.99, dtype=float32),\n",
            "              array(104.99, dtype=float32),\n",
            "              array(104.99, dtype=float32),\n",
            "              array(104.99, dtype=float32),\n",
            "              array(104.99, dtype=float32),\n",
            "              array(104.99, dtype=float32),\n",
            "              array(104.99, dtype=float32),\n",
            "              array(104.99, dtype=float32),\n",
            "              array(104.99, dtype=float32),\n",
            "              array(104.99, dtype=float32)]},\n",
            " {'observations': [67.18574479266941,\n",
            "                   68.85869234640361,\n",
            "                   68.1847721008489,\n",
            "                   65.75546474673988,\n",
            "                   68.58260215891899,\n",
            "                   68.1176324531035,\n",
            "                   68.18644487041036,\n",
            "                   68.15681716077954,\n",
            "                   65.26703019486334,\n",
            "                   68.77233251796105,\n",
            "                   66.1437325647721],\n",
            "  'rewards': [array(67.24435, dtype=float32),\n",
            "              array(68.858696, dtype=float32),\n",
            "              array(68.18477, dtype=float32),\n",
            "              array(66.34543, dtype=float32),\n",
            "              array(68.5826, dtype=float32),\n",
            "              array(68.11763, dtype=float32),\n",
            "              array(68.72748, dtype=float32),\n",
            "              array(68.156815, dtype=float32),\n",
            "              array(66.47513, dtype=float32),\n",
            "              array(68.77233, dtype=float32)]},\n",
            " {'observations': [69.0,\n",
            "                   69.0,\n",
            "                   69.0,\n",
            "                   69.0,\n",
            "                   69.0,\n",
            "                   69.0,\n",
            "                   69.0,\n",
            "                   69.0,\n",
            "                   69.0,\n",
            "                   69.0,\n",
            "                   69.0],\n",
            "  'rewards': [array(69., dtype=float32),\n",
            "              array(69., dtype=float32),\n",
            "              array(69., dtype=float32),\n",
            "              array(69., dtype=float32),\n",
            "              array(69., dtype=float32),\n",
            "              array(69., dtype=float32),\n",
            "              array(69., dtype=float32),\n",
            "              array(69., dtype=float32),\n",
            "              array(69., dtype=float32),\n",
            "              array(69., dtype=float32)]},\n",
            " {'observations': [69.99,\n",
            "                   69.99,\n",
            "                   69.99,\n",
            "                   69.99,\n",
            "                   69.99,\n",
            "                   69.99,\n",
            "                   69.99,\n",
            "                   69.99,\n",
            "                   69.99,\n",
            "                   69.99,\n",
            "                   69.99],\n",
            "  'rewards': [array(69.99, dtype=float32),\n",
            "              array(69.99, dtype=float32),\n",
            "              array(69.99, dtype=float32),\n",
            "              array(69.99, dtype=float32),\n",
            "              array(69.99, dtype=float32),\n",
            "              array(69.99, dtype=float32),\n",
            "              array(69.99, dtype=float32),\n",
            "              array(69.99, dtype=float32),\n",
            "              array(69.99, dtype=float32),\n",
            "              array(69.99, dtype=float32)]},\n",
            " {'observations': [66.99,\n",
            "                   66.99,\n",
            "                   66.99,\n",
            "                   66.99,\n",
            "                   66.99,\n",
            "                   66.99,\n",
            "                   66.99,\n",
            "                   66.99,\n",
            "                   66.99,\n",
            "                   66.99,\n",
            "                   66.99],\n",
            "  'rewards': [array(66.99, dtype=float32),\n",
            "              array(66.99, dtype=float32),\n",
            "              array(66.99, dtype=float32),\n",
            "              array(66.99, dtype=float32),\n",
            "              array(66.99, dtype=float32),\n",
            "              array(66.99, dtype=float32),\n",
            "              array(66.99, dtype=float32),\n",
            "              array(66.99, dtype=float32),\n",
            "              array(66.99, dtype=float32),\n",
            "              array(66.99, dtype=float32)]}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq_K9yBjYPQj",
        "outputId": "299bb7e7-8535-408e-e4fb-25168c0b66f5"
      },
      "source": [
        "environment = MultiArmedBanditPyEnvironment()\n",
        "observation = environment.reset().observation\n",
        "print(f\"{observation = }\")\n",
        "\n",
        "action = 2\n",
        "print(f\"{action = }\")\n",
        "reward = environment.step(action).reward\n",
        "print(f\"{reward = }\")"
      ],
      "id": "Nq_K9yBjYPQj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "observation = array([0])\n",
            "action = 2\n",
            "reward = array([0.], dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSuZrUelYPQk"
      },
      "source": [
        "tf_environment = tf_py_environment.TFPyEnvironment(environment)"
      ],
      "id": "MSuZrUelYPQk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcOnRsmYYPQl"
      },
      "source": [
        "class SignPolicy(tf_policy.TFPolicy):\n",
        "    def __init__(self):\n",
        "        observation_spec = tensor_spec.BoundedTensorSpec(\n",
        "            shape=(1,), dtype=tf.int32, minimum=-2, maximum=2)\n",
        "        time_step_spec = ts.time_step_spec(observation_spec)\n",
        "\n",
        "        action_spec = tensor_spec.BoundedTensorSpec(\n",
        "            shape=(), dtype=tf.int32, minimum=0, maximum=2)\n",
        "\n",
        "        super(SignPolicy, self).__init__(time_step_spec=time_step_spec,\n",
        "                                        action_spec=action_spec)\n",
        "    def _distribution(self, time_step):\n",
        "        pass\n",
        "\n",
        "    def _variables(self):\n",
        "        return ()\n",
        "\n",
        "    def _action(self, time_step, policy_state, seed):\n",
        "        observation_sign = tf.cast(tf.sign(time_step.observation[0]), dtype=tf.int32)\n",
        "        action = observation_sign + 1\n",
        "        return policy_step.PolicyStep(action, policy_state)"
      ],
      "id": "OcOnRsmYYPQl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShrR8HsqYPQl",
        "outputId": "fdc530a5-9537-4414-bc81-559fe5e873f2"
      },
      "source": [
        "sign_policy = SignPolicy()\n",
        "\n",
        "current_time_step = tf_environment.reset()\n",
        "print(f'{current_time_step.observation = }')\n",
        "\n",
        "action = sign_policy.action(current_time_step).action\n",
        "print(f'{action = }')\n",
        "reward = tf_environment.step(action).reward\n",
        "print(f'{reward = }')"
      ],
      "id": "ShrR8HsqYPQl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current_time_step.observation = <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]])>\n",
            "action = <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>\n",
            "reward = <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}